{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Huawei for job.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e64525a510a14588abfea151322f1214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a22e200927e946c79fc291cf8b209815",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1c099ba8985e4e4997bd95f7f3c5cf17",
              "IPY_MODEL_adf13b05a22f4ce3a31c950dd6c6143e"
            ]
          }
        },
        "a22e200927e946c79fc291cf8b209815": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1c099ba8985e4e4997bd95f7f3c5cf17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_59e95d0cc9574a6ba46c02d667f1dec8",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 51,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 51,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cdd31069219449798442475d0429f236"
          }
        },
        "adf13b05a22f4ce3a31c950dd6c6143e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0e6c43896cff4ab7b0b663ed98f90cc4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 51/51 [00:00&lt;00:00, 2789.38it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bca7f74bb99f4ee1ab224dd01f85c98d"
          }
        },
        "59e95d0cc9574a6ba46c02d667f1dec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cdd31069219449798442475d0429f236": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0e6c43896cff4ab7b0b663ed98f90cc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bca7f74bb99f4ee1ab224dd01f85c98d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvGZqhPBviSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVMdoe3spXO6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "59232c6f-0fa8-4775-d128-67ceec807ac5"
      },
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm_notebook, tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "from IPython.display import clear_output\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ol_f9ADbgIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#os.chdir('/content/drive/My Drive/Huawei_for_job')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENisejm4flkV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99c58c9f-48a0-4e02-9517-b12b3ac38986"
      },
      "source": [
        "punctuation = set('!,.-?:;')\n",
        "nothing = ''\n",
        "labels = [nothing] + list(punctuation)\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "le.fit(labels)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LabelEncoder()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk9CUdB-zcvj",
        "colab_type": "text"
      },
      "source": [
        "### main part"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg_m3_sGRrY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = transformers.BertTokenizer.from_pretrained('DeepPavlov/rubert-base-cased')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgfU6uulz4Mt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_encoding(data_path):\n",
        "  with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    data = f.readlines()\n",
        "\n",
        "  X = []\n",
        "  Y = []\n",
        "\n",
        "  for line in tqdm_notebook(data):\n",
        "    word, punc = line.split('\\t')\n",
        "    if word in string.punctuation:\n",
        "      continue\n",
        "    punc = punc.strip()\n",
        "    tokens = tokenizer.tokenize(word)\n",
        "    x = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    y = [int(punc)]\n",
        "    if len(x) > 0:\n",
        "      if len(x) > 1:\n",
        "        y = (len(x)-1)*[0]+y\n",
        "      X += x\n",
        "      Y += y\n",
        "  return X, Y"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-u_govZORrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('data_X.pickle', 'rb') as f:\n",
        "  X = pickle.load(f)\n",
        "\n",
        "with open('data_y.pickle', 'rb') as f:\n",
        "  y = pickle.load(f)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46A_gPzmCM79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEGMENT_SIZE = 32\n",
        "\n",
        "def create_segments(x, segment_size = SEGMENT_SIZE):\n",
        "    X = []\n",
        "    #для первого и последнего слова добавляем слова\n",
        "    x_pad = x[-((segment_size-1)//2-1):]+x+x[:segment_size//2]\n",
        "\n",
        "    for i in tqdm_notebook(range(len(x_pad)-segment_size+2)):\n",
        "        segment = x_pad[i:i+segment_size-1]\n",
        "        X.append(segment)\n",
        "\n",
        "    return np.array(X)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niWlYwp6bKbT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from utils import check_one, gold_str\n",
        "\n",
        "# функция для проверки пунктуации\n",
        "def check_metric(model, device):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  tokenized_input = list(filter(lambda x: x not in string.punctuation, word_tokenize(gold_str)))\n",
        "  tokens = tokenizer.convert_tokens_to_ids(tokenized_input)\n",
        "  segments = torch.tensor(create_segments(tokens), dtype = torch.long).to(device)\n",
        "  outputs = le.inverse_transform(model(segments).argmax(dim=-1).cpu()).tolist()\n",
        "\n",
        "  i = 0\n",
        "  j = 0\n",
        "  while j<len(outputs):\n",
        "    tokenized_input.insert(i+1, outputs[j])\n",
        "    i += 2\n",
        "    j += 1\n",
        "\n",
        "  hypothesis = ' '.join(tokenized_input)\n",
        "  print(hypothesis)\n",
        "  return check_one(gold_str, hypothesis)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LvtKdctg0n_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "check_metric(model, torch.device('cuda'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8EqpDaCCJTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "class BertPunc(nn.Module):  \n",
        "    \n",
        "    def __init__(self, dropout, seq_len = SEGMENT_SIZE):\n",
        "        super(BertPunc, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('DeepPavlov/rubert-base-cased')\n",
        "        self.bert_vocab_size = tokenizer.vocab_size\n",
        "        self.bn = nn.BatchNorm1d((seq_len-1)*768)\n",
        "        self.fc1 = nn.Linear((seq_len-1)*768, ((seq_len-1)*768)//2)\n",
        "        self.fc2 = nn.Linear(((seq_len-1)*768)//2, len(labels))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input_ids = None):\n",
        "        x = self.bert(input_ids = input_ids)[0]\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc2(self.dropout(F.relu(self.fc1(self.dropout(self.bn(x))))))\n",
        "        return x"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH00woHfGZl1",
        "colab_type": "text"
      },
      "source": [
        "### Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDUN9rFSGc6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# under sampling\n",
        "full_indexes = []\n",
        "y = np.array(y)\n",
        "n_samples = 20000000\n",
        "max_len = 80000\n",
        "for i in tqdm_notebook(range(8)):\n",
        "  indexes = []\n",
        "  for idx, j in enumerate(y[:n_samples]):\n",
        "    if len(indexes) >= max_len:\n",
        "      continue\n",
        "    if j == i:\n",
        "      indexes.append(idx)\n",
        "  full_indexes += indexes\n",
        "\n",
        "full_indexes = np.array(full_indexes)\n",
        "X = create_segments(X[:n_samples])\n",
        "X = X[full_indexes]\n",
        "y = y[full_indexes]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkWqzI4Gw0wX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = X[:500000]\n",
        "y = y[:500000]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7eYjOBy1gkn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.1, stratify = y)\n",
        "X_train = torch.from_numpy(X_train).long()\n",
        "X_val = torch.from_numpy(X_val).long()\n",
        "y_train = torch.tensor(y_train, dtype = torch.long)\n",
        "y_val = torch.tensor(y_val, dtype = torch.long)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFwQaoMRC2kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "val_dataset = TensorDataset(X_val, y_val)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrA9r7pE1oTk",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhFTkbp49vhG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, dataloader, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    epoch_loss = 0.\n",
        "    acc = 0.\n",
        "    with tqdm(desc=\"batch\", total=len(dataloader)) as pbar_outer:\n",
        "      for i, batch in enumerate(dataloader, 1):\n",
        "\n",
        "            optimizer.zero_grad()     \n",
        "\n",
        "            x, labels = batch    \n",
        "            x = x.to(device)\n",
        "            labels = labels.to(device)             \n",
        "            \n",
        "            outputs = model(x)\n",
        "            \n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            loss.backward()\n",
        "            \n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            \n",
        "            optimizer.step()\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            acc += metrics.accuracy_score(labels.cpu().data.numpy().flatten(), outputs.argmax(dim=1).cpu().data.numpy().flatten())\n",
        "\n",
        "            if i%100 == 0:\n",
        "              pbar_outer.update(100)\n",
        "              tqdm.write(\"train_loss:{}\".format(loss.item()))\n",
        "\n",
        "            if i%5000 == 0:\n",
        "              torch.save(model.state_dict(), 'puncBert_{}.pth'.format(i))\n",
        "        \n",
        "    train_loss = round( (epoch_loss / len(dataloader)), 3)\n",
        "    train_acc = round( (acc / len(dataloader)), 3)\n",
        "        \n",
        "    return train_loss, train_acc"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TJoakJfBWP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(model, dataloader, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    acc = 0.\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      with tqdm(desc=\"batch\", total=len(dataloader)) as pbar_outer:\n",
        "        for i, batch in enumerate(dataloader, 1):\n",
        "\n",
        "            x, labels = batch    \n",
        "            x = x.to(device)\n",
        "            labels = labels.to(device)             \n",
        "            \n",
        "            outputs = model(x)\n",
        "        \n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "            acc += metrics.accuracy_score(labels.cpu().data.numpy().flatten(), outputs.argmax(dim=1).cpu().data.numpy().flatten())\n",
        "            \n",
        "            if i%100 == 0:\n",
        "              pbar_outer.update(100)\n",
        "              tqdm.write(\"val_loss:{}  val_acc:{}\".format(loss.item(), acc/i))\n",
        "            \n",
        "        valid_loss = round((epoch_loss / len(dataloader)), 3)\n",
        "        metric = check_metric(model, device)\n",
        "        val_acc = round( (acc / len(dataloader)), 3)\n",
        "  \n",
        "    return valid_loss, metric, val_acc"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oDw53P1BYmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "\n",
        "device = torch.device('cuda')\n",
        "#metrics = -100\n",
        "\n",
        "model = BertPunc(0.3).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, 0.75)\n",
        "\n",
        "clip = 3\n",
        "num_epochs = 50\n",
        "best_loss = 100\n",
        "history_val = []\n",
        "history_train = []\n",
        "\n",
        "for epoch in tqdm_notebook(range(num_epochs)):\n",
        "  \n",
        "  train_loss, train_acc = train(model, train_dataloader, optimizer, criterion, clip)\n",
        "  valid_loss, metric, val_acc = evaluate(model, val_dataloader, criterion)\n",
        "  scheduler.step()\n",
        "\n",
        "  history_val.append(valid_loss)\n",
        "  history_train.append(train_loss)\n",
        "\n",
        "  clear_output(True)\n",
        "  plt.plot(history_train, label = 'train_loss')\n",
        "  plt.plot(history_val, label = 'valid_loss')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "  time.sleep(3)\n",
        "\n",
        "\n",
        "  print('Epoch: {} \\n Train Loss {}  Val loss {}  Train acuracy {}  Val acuracy {}  Metric {}'.format(epoch + 1, train_loss, valid_loss, train_acc, val_acc, metric))\n",
        "\n",
        "  if valid_loss < best_loss:\n",
        "    best_loss = valid_loss\n",
        "    torch.save(model.state_dict(), 'puncBert.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhg8zvI3NaQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = next(iter(val_dataloader))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfBT_NPuNs8p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "e64525a510a14588abfea151322f1214",
            "a22e200927e946c79fc291cf8b209815",
            "1c099ba8985e4e4997bd95f7f3c5cf17",
            "adf13b05a22f4ce3a31c950dd6c6143e",
            "59e95d0cc9574a6ba46c02d667f1dec8",
            "cdd31069219449798442475d0429f236",
            "0e6c43896cff4ab7b0b663ed98f90cc4",
            "bca7f74bb99f4ee1ab224dd01f85c98d"
          ]
        },
        "outputId": "9d1accf9-4ad4-4cd6-9f63-6014a5d87014"
      },
      "source": [
        "check_metric(model, device)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e64525a510a14588abfea151322f1214",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=51.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Начиная  жизнеописание , героя ? моего , Алексея , Федоровича ? Карамазова , нахожусь  в  некотором , недоумении . А  именно : хотя  я , и  называю : Алексея , Федоровича : моим  героем . но , однако , сам  знаю , что : человек . он , отнюдь , не  великий , а  посему , и  предвижу . неизбежные . вопросы , вроде : таковых : чем  же ? замечателен ? ваш ? Алексей , Федорович , что ? вы ? выбрали ? его ? своим  героем ?\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8771929824561403"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFctpqyMSW1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tqdm._instances.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o24w7l1aj30p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7SmcNfdgknIN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "0c91070e-5362-4a88-cc14-a79f125871ca"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jul 19 22:38:30 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.51.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    33W / 250W |  16265MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHtmroockoW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}